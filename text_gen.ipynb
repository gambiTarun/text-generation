{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open(data).read()\n",
    "# lets look at the text\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Number of unique chars: 65\n"
     ]
    }
   ],
   "source": [
    "# set-> Build an unordered collection of unique elements.\n",
    "vocab = sorted(set(text))\n",
    "print(vocab)\n",
    "print('Number of unique chars: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a mapping from unique chars to index and vice versa\n",
    "idx_from_char = {u:i for i,u in enumerate(vocab)}\n",
    "char_from_idx = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([idx_from_char[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen ------mapped--to-----> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print('{} ------mapped--to-----> {}'.format(text[:13], text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# Creating training samples\n",
    "# the maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "\n",
    "# breaking the text into chunks of seq_lenght+1\n",
    "chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "# text broken into chunks of seq_lenght+1 parts to create training and target data\n",
    "for item in chunks.take(5):\n",
    "    print(repr(''.join(char_from_idx[item.numpy()])))\n",
    "    \n",
    "# example: \n",
    "# chunk = 'Apple' (seq_lenght+1 = 5)\n",
    "# training point = 'Appl'\n",
    "# target point = 'pple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_target(chunk):\n",
    "    train = chunk[:-1]\n",
    "    target = chunk[1:]\n",
    "    return train, target\n",
    "\n",
    "dataset = chunks.map(split_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input string:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "target string:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for in_example, out_example in dataset.take(1):\n",
    "    print('input string: ', repr(''.join(char_from_idx[in_example.numpy()])))\n",
    "    print('target string: ', repr(''.join(char_from_idx[out_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0\n",
      "    input: 18 (F)\n",
      "    expected out: 47 (i)\n",
      "Step  1\n",
      "    input: 47 (i)\n",
      "    expected out: 56 (r)\n",
      "Step  2\n",
      "    input: 56 (r)\n",
      "    expected out: 57 (s)\n",
      "Step  3\n",
      "    input: 57 (s)\n",
      "    expected out: 58 (t)\n",
      "Step  4\n",
      "    input: 58 (t)\n",
      "    expected out: 1 ( )\n"
     ]
    }
   ],
   "source": [
    "for i,(in_idx, out_idx) in enumerate(zip(in_example[:5],out_example[:5])):\n",
    "    print('Step {:2d}'.format(i))\n",
    "    print('    input: {} ({:s})'.format(in_idx,char_from_idx[in_idx]))\n",
    "    print('    expected out: {} ({:s})'.format(out_idx,char_from_idx[out_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dataset shape:  (64, 100)\n",
      "target dataset shape:  (64, 100)\n"
     ]
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset_shuffled = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# text broken into chunks of seq_lenght+1 parts to create training and target data\n",
    "# then dataset of 'in' and 'out' created\n",
    "# now SHUFFLED\n",
    "for in_example, out_example in dataset_shuffled.take(1):\n",
    "    print('input dataset shape: ', in_example.shape)\n",
    "    print('target dataset shape: ', out_example.shape)\n",
    "    # This Shape will be the inpput and output shape of our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MODEL (using Functional API)\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self,vocab_size, embedding_dim, units):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding  = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       return_sequences=True,\n",
    "                                       recurrent_initializer='glorot_uniform',\n",
    "                                       stateful=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "        output = self.gru(embedding)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction\n",
    "\n",
    "vocab_size = len(vocab)   # (y=[vocab_size,1], Wya=[vocab_size,units])\n",
    "embedding_dim = 256       # (x=[embedding_dim,1], Wax=[embedding_dim,units])\n",
    "units = 1024              # units are the number of nodes in the kernel layer (a=[units,1] ,Waa=[units,units])\n",
    "\n",
    "model = Model(vocab_size,embedding_dim,units)\n",
    "\n",
    "# OR\n",
    "#\n",
    "# This approach requires you to enter the specifications of the \n",
    "# model (if there, like number of units in wach layer) with the \n",
    "# inputs when creating a model \n",
    "# \n",
    "# The above method can solve this problem by using a constructor to use the \n",
    "# specifications when creating an instance of the model, which later can be\n",
    "# called with inputs to create the model. \n",
    "# \n",
    "# def Model_2(inputs,vocab_size,embedding_dim,units):\n",
    "#     embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "#     x = tf.keras.layers.GRU(units,\n",
    "#                                  recurrent_activation='sigmoid',\n",
    "#                                  return_sequences=True,\n",
    "#                                  recurrent_initializer='glorot_uniform',\n",
    "#                                  stateful=True)(embedding)\n",
    "#     outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
    "#     model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "# model = Model_2(inputs,vocab_size,embedding_dim,units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# using sparse_.. so that we do not nedd to convert to one-hot vectors\n",
    "def loss_func(real,pred):\n",
    "    return  tf.losses.sparse_softmax_cross_entropy(labels=real, logits=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above method of using a SubClass to instantiate the Model\n",
    "# require us to 'build' to show the shape of the input layer\n",
    "\n",
    "model.build(tf.TensorShape([BATCH_SIZE, seq_length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  multiple                  3935232   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  66625     \n",
      "=================================================================\n",
      "Total params: 4,018,497\n",
      "Trainable params: 4,018,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding/embeddings:0' shape=(65, 256) dtype=float32, numpy=\n",
       " array([[ 0.00512834,  0.0436937 ,  0.02530426, ...,  0.04631868,\n",
       "          0.04996283, -0.01745743],\n",
       "        [ 0.0203251 ,  0.00948094, -0.0430881 , ...,  0.02432409,\n",
       "         -0.0276683 ,  0.00305264],\n",
       "        [ 0.02338693,  0.00400842, -0.01564399, ...,  0.01133979,\n",
       "          0.03369172, -0.00371819],\n",
       "        ...,\n",
       "        [ 0.01701743,  0.03860707, -0.03241114, ...,  0.0488064 ,\n",
       "          0.01927694, -0.0384011 ],\n",
       "        [ 0.03603819, -0.01086976, -0.04505264, ...,  0.02887638,\n",
       "         -0.0352305 ,  0.04337858],\n",
       "        [-0.02075713, -0.02915847,  0.03976054, ...,  0.02994708,\n",
       "          0.01739   ,  0.00872176]], dtype=float32)>,\n",
       " <tf.Variable 'gru/kernel:0' shape=(256, 3072) dtype=float32, numpy=\n",
       " array([[-0.02622047,  0.01068696, -0.02080663, ...,  0.02202471,\n",
       "          0.0102331 , -0.03938706],\n",
       "        [-0.00127871,  0.03858697,  0.03139468, ..., -0.04026121,\n",
       "         -0.04024152,  0.02658733],\n",
       "        [ 0.02317402, -0.02839136, -0.01188014, ...,  0.010361  ,\n",
       "          0.03396414, -0.02079866],\n",
       "        ...,\n",
       "        [ 0.02532004,  0.00089138,  0.00924547, ...,  0.0108701 ,\n",
       "         -0.01985942,  0.02424383],\n",
       "        [-0.0177195 , -0.04212833, -0.02689726, ..., -0.01924209,\n",
       "          0.03863584,  0.02464545],\n",
       "        [ 0.01774756, -0.00155659, -0.0424182 , ...,  0.01821718,\n",
       "         -0.01830764, -0.0269535 ]], dtype=float32)>,\n",
       " <tf.Variable 'gru/recurrent_kernel:0' shape=(1024, 3072) dtype=float32, numpy=\n",
       " array([[-0.02052492,  0.01924744, -0.0145707 , ..., -0.01761001,\n",
       "         -0.02813859, -0.019577  ],\n",
       "        [-0.01691091,  0.02702141,  0.02052569, ...,  0.0105403 ,\n",
       "         -0.00245157,  0.00502136],\n",
       "        [ 0.00352367, -0.00998699,  0.02923493, ..., -0.02451134,\n",
       "          0.03027378, -0.02624994],\n",
       "        ...,\n",
       "        [ 0.01894793, -0.03167696,  0.01855785, ...,  0.0130463 ,\n",
       "          0.02036413, -0.01859545],\n",
       "        [ 0.03340564, -0.01860097,  0.00734494, ...,  0.02523733,\n",
       "         -0.01332495, -0.02530179],\n",
       "        [-0.02779041, -0.03081418, -0.00867376, ...,  0.00111116,\n",
       "         -0.00104531, -0.02393005]], dtype=float32)>,\n",
       " <tf.Variable 'gru/bias:0' shape=(3072,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(1024, 65) dtype=float32, numpy=\n",
       " array([[ 0.03523315,  0.05288604, -0.0515499 , ..., -0.04223362,\n",
       "          0.01809403,  0.06218062],\n",
       "        [ 0.07170269,  0.01071525,  0.00665887, ...,  0.00201895,\n",
       "         -0.03331507, -0.01711798],\n",
       "        [ 0.04603922,  0.00455201, -0.02251478, ...,  0.02795295,\n",
       "         -0.03044961, -0.06886444],\n",
       "        ...,\n",
       "        [-0.01596258, -0.0339536 ,  0.01950633, ..., -0.05147813,\n",
       "          0.01854692,  0.03120432],\n",
       "        [ 0.03388858,  0.05666213, -0.05661613, ...,  0.06534393,\n",
       "          0.07058215, -0.02203514],\n",
       "        [ 0.01141271,  0.07098506, -0.07362213, ...,  0.05501895,\n",
       "          0.06476983,  0.06590118]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(65,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = 'C:/Users/Tarunbir Singh/Documents/Machine Learning/text_generation/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt')\n",
    "# Checkpoint instance\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-88-a795d74e0f42>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-88-a795d74e0f42>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    start = time.time()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initialising the hidden state at the start of every epoch\n",
    "    # initial hidden state is None\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (inp,target)) in enumerate(dataset_shuffled):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # feeding the hidden state back into the model\n",
    "            predictions = model(inp)\n",
    "            loss = loss_func(target, predictions)\n",
    "            \n",
    "        # gradients for back-propagation using Gradient Taping\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        # applying the updation of variables using the calcuated grads\n",
    "        optimizer.apply_gradients(zip(grads,model.variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,batch,loss))\n",
    "            \n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch+1)%5==0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model needs to train only once\n",
    "# can be restored from the checkpoints created\n",
    "model = Model(vocab_size, embedding_dim, units)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f you mistoking and she falls.\n",
      "\n",
      "PAULINA:\n",
      "There's comfort's bloody throne of it!\n",
      "Twenty-challenges, where we were alive,\n",
      "For John of Gaunt indeed, I will understand thee from so suffer in.\n",
      "Where you we'll find you know\n",
      "That you thing entle than a man of victory.\n",
      "Mayac, leaving this?\n",
      "Of love be done. Good morrow, fellow.\n",
      "\n",
      "FLORIZALET:\n",
      "The mailing of our other sights.\n",
      "You are made, my liege,\n",
      "The warrants of a woman's humour.\n",
      "\n",
      "Vessenger:\n",
      "There is a man-form load: you must strike?\n",
      "\n",
      "Shepherd:\n",
      "Ay, sir.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Ay, What work he was?\n",
      "Hark you, that Margaret's battle are abaded,\n",
      "For valiant Clifford, with his state, our justice comes to fight:\n",
      "And I have noned from your feeding, become my saying,\n",
      "HADY ANNE:\n",
      "And, sir, he for his mean: madam, safe, that is not so remedy.\n",
      "\n",
      "FRIARNLLO:\n",
      "Why with thy cogonax,\n",
      "From my remembrance would have blush up my thoughty af her head, I swear,\n",
      "But one that are near youd?\n",
      "\n",
      "Second Keeper:\n",
      "Ind brought ut ill:\n",
      "And every cager, therein flosts do wrink so unprov\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Step\n",
    "\n",
    "# number of characters to generate\n",
    "num_generate = 1000\n",
    "\n",
    "# The Starting String input, \n",
    "# can be experimented\n",
    "starting_string = 'f'\n",
    "\n",
    "# convert to index our starting string\n",
    "input_eval = [idx_from_char[i] for i in starting_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "# variable to store the generated text\n",
    "text_generated = []\n",
    "\n",
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "# Experiment to find the best setting.\n",
    "temperature = 1.0\n",
    "\n",
    "# here batch size == 1\n",
    "model.reset_states()\n",
    "for i in range(num_generate):\n",
    "    prediction = model(input_eval)\n",
    "    # remove the batch dimensions (flatten)\n",
    "    prediction = tf.squeeze(prediction, 0)\n",
    "    \n",
    "    # using a multinomial distribution to predict the word returned by the model\n",
    "    prediction = prediction / temperature\n",
    "    predicted_id = tf.multinomial(prediction, num_samples=1)[-1,0].numpy()\n",
    "    \n",
    "    # we pass the predicted word as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id],0)\n",
    "    \n",
    "    text_generated.append(char_from_idx[predicted_id])\n",
    "    \n",
    "print(starting_string + ''.join(text_generated))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
